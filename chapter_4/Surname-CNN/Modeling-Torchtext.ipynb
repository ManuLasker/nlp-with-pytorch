{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "import json\n",
    "import os \n",
    "import re\n",
    "import string\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "from torchtext.data import (Dataset, Field, LabelField, \n",
    "                            BucketIterator, Iterator, Example) \n",
    "from tqdm.notebook import tqdm as tqdm_notebook\n",
    "from argparse import Namespace\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "writer = SummaryWriter()\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Args Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Namespace(\n",
    "    # Data Path and Information\n",
    "    surnames_csv=\"data/surnames/surnames_with_splits.csv\",\n",
    "    model_state_file=\"model.pth\",\n",
    "    save_dir=\"model_storage\",\n",
    "    # Model hyper parameters\n",
    "    # Model hyper parameters\n",
    "    hidden_dim=100,\n",
    "    num_channels=256,\n",
    "    # Training hyper parameters\n",
    "    seed=1337,\n",
    "    learning_rate=0.001,\n",
    "    batch_size=128,\n",
    "    num_epochs=100,\n",
    "    early_stopping_criteria=5,\n",
    "    dropout_p=0.1,\n",
    "    # Runtime options\n",
    "    catch_keyboard_interrupt=True,\n",
    "    expand_filepaths_to_save_dir=True,\n",
    "    reload_from_files=False,\n",
    "    cuda=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expanded filepaths: \n",
      "\tmodel_storage/model.pth\n",
      "Using CUDA: False\n"
     ]
    }
   ],
   "source": [
    "def set_seed_everywhere(seed, cuda):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if cuda:\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def handle_dirs(dirpath):\n",
    "    if not os.path.exists(dirpath):\n",
    "        os.makedirs(dirpath)\n",
    "\n",
    "if args.expand_filepaths_to_save_dir:\n",
    "    args.model_state_file = os.path.join(args.save_dir,\n",
    "                                         args.model_state_file)\n",
    "    \n",
    "    print(\"Expanded filepaths: \")\n",
    "    print(\"\\t{}\".format(args.model_state_file))\n",
    "    \n",
    "# Check CUDA\n",
    "if not torch.cuda.is_available():\n",
    "    args.cuda = False\n",
    "\n",
    "print(\"Using CUDA: {}\".format(args.cuda))\n",
    "\n",
    "args.device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
    "\n",
    "# Set seed for reproducibility\n",
    "set_seed_everywhere(args.seed, args.cuda)\n",
    "\n",
    "# handle dirs\n",
    "handle_dirs(args.save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "surnames = pd.read_csv(args.surnames_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>surname</th>\n",
       "      <th>nationality</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Lovell</td>\n",
       "      <td>English</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Nader</td>\n",
       "      <td>Arabic</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Avertchenko</td>\n",
       "      <td>Russian</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Yim</td>\n",
       "      <td>Korean</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Balanev</td>\n",
       "      <td>Russian</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       surname nationality  split\n",
       "0       Lovell     English  train\n",
       "1        Nader      Arabic  train\n",
       "2  Avertchenko     Russian  train\n",
       "3          Yim      Korean  train\n",
       "4      Balanev     Russian  train"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "surnames.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TorchText Fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(text):\n",
    "    text = [char for char in text]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "SURNAME = Field(sequential=True, tokenize=tokenizer, lower=False, fix_length=17)\n",
    "NATIONALITY = LabelField(dtype=torch.float32, lower=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TorchText Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SurnamesDataset(Dataset):\n",
    "    def __init__(self, surnames_df, fields):\n",
    "        examples = []\n",
    "        for idx, row in tqdm_notebook(surnames_df.iterrows(), \n",
    "                                      total=surnames_df.shape[0]):\n",
    "            surname = row.surname\n",
    "            nationality = row.nationality\n",
    "            examples.append(Example.fromlist([surname, nationality], fields))\n",
    "        super().__init__(examples, fields)\n",
    "            \n",
    "    @staticmethod\n",
    "    def sort_key(data):\n",
    "        return len(data.surname)\n",
    "    \n",
    "    @classmethod\n",
    "    def splits(cls, fields, surnames_df):\n",
    "        train_data = cls(surnames_df[surnames_df.split == \"train\"], fields)\n",
    "        val_data = cls(surnames_df[surnames_df.split == \"val\"], fields)\n",
    "        test_data = cls(surnames_df[surnames_df.split == \"test\"], fields)\n",
    "        return train_data, val_data, test_data        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d819d5fc2b2949b093019ea5c3b33ca9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=7685.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3fe21afbadf4708ab908c9d2e3a33ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1647.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41692e418a7a47509df8655a5ff917ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1648.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "fields = [('surname', SURNAME), ('nationality', NATIONALITY)]\n",
    "train_data, val_data, test_data = SurnamesDataset.splits(fields, surnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'surname': ['L', 'o', 'v', 'e', 'l', 'l'], 'nationality': 'english'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vars(train_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "SURNAME.build_vocab(train_data, min_freq=0)\n",
    "NATIONALITY.build_vocab(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['freqs', 'itos', 'unk_index', 'stoi', 'vectors'])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vars(SURNAME.vocab).keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<unk>', '<pad>']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SURNAME.vocab.itos[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "84"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(SURNAME.vocab.itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['english',\n",
       " 'russian',\n",
       " 'arabic',\n",
       " 'japanese',\n",
       " 'italian',\n",
       " 'german',\n",
       " 'czech',\n",
       " 'spanish',\n",
       " 'dutch',\n",
       " 'french',\n",
       " 'chinese',\n",
       " 'irish',\n",
       " 'greek',\n",
       " 'polish',\n",
       " 'korean',\n",
       " 'scottish',\n",
       " 'vietnamese',\n",
       " 'portuguese']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NATIONALITY.vocab.itos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One Hot Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vectorizer(object):\n",
    "    def __init__(self, surname_field):\n",
    "        self._surname_field = surname_field\n",
    "        self.slicing = torch.tensor([i for i in range(len(self._surname_field.vocab.itos)) if i!=1])\n",
    "        self.num_characters = len(self._surname_field.vocab.itos)\n",
    "        self._dimension = len(self.slicing)\n",
    "    \n",
    "    def vectorize(self, batch_matrix):\n",
    "        one_hot_sequence = F.one_hot(batch_matrix.T, self.num_characters)\n",
    "        #one_hot_sequence = one_hot_sequence[:, :, self.slicing]\n",
    "        return one_hot_sequence.transpose(2,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "oh_vectorizer = Vectorizer(SURNAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Iterators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iterator, val_iterator = BucketIterator.splits(\n",
    "    (train_data, val_data), \n",
    "    batch_size = args.batch_size,\n",
    "    sort_within_batch = True,\n",
    "    device = args.device,\n",
    ")\n",
    "\n",
    "test_iterator = Iterator(test_data, batch_size=2,\n",
    "                        device=args.device, sort=False, sort_within_batch=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 10, 7])\n",
      "torch.Size([2, 16, 5])\n",
      "torch.Size([2, 16, 5])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 2\n",
    "one_hot_size = 10\n",
    "sequence_width = 7\n",
    "data = torch.randn(batch_size, one_hot_size, sequence_width)\n",
    "conv1 = nn.Conv1d(in_channels=one_hot_size,\n",
    "                 out_channels=16,\n",
    "                 kernel_size=3)\n",
    "intermediate = conv1(data)\n",
    "print(data.size())\n",
    "print(intermediate.squeeze(2).size())\n",
    "print(intermediate.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv1 = nn.Conv1d(in_channels=84,\n",
    "                 out_channels=16,\n",
    "                 kernel_size=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SurnameClassifier(nn.Module):\n",
    "    def __init__(self, initial_num_channels, num_classes, num_channels):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            initial_num_channels (int): size of the incoming feature vector\n",
    "            num_classes (int): size of the output prediction vector\n",
    "            num_channels (int): constant channel size to use throughout network\n",
    "        \"\"\"\n",
    "        super(SurnameClassifier, self).__init__()\n",
    "        \n",
    "        self.convnet = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=initial_num_channels, \n",
    "                      out_channels=num_channels, kernel_size=3),\n",
    "            nn.ELU(),\n",
    "            nn.Conv1d(in_channels=num_channels, out_channels=num_channels, \n",
    "                      kernel_size=3, stride=2),\n",
    "            nn.ELU(),\n",
    "            nn.Conv1d(in_channels=num_channels, out_channels=num_channels, \n",
    "                      kernel_size=3, stride=2),\n",
    "            nn.ELU(),\n",
    "            nn.Conv1d(in_channels=num_channels, out_channels=num_channels, \n",
    "                      kernel_size=3),\n",
    "            nn.ELU()\n",
    "        )\n",
    "        self.fc = nn.Linear(num_channels, num_classes)\n",
    "\n",
    "    def forward(self, x_surname, apply_softmax=False):\n",
    "        \"\"\"The forward pass of the classifier\n",
    "        \n",
    "        Args:\n",
    "            x_surname (torch.Tensor): an input data tensor. \n",
    "                x_surname.shape should be (batch, initial_num_channels, max_surname_length)\n",
    "            apply_softmax (bool): a flag for the softmax activation\n",
    "                should be false if used with the Cross Entropy losses\n",
    "        Returns:\n",
    "            the resulting tensor. tensor.shape should be (batch, num_classes)\n",
    "        \"\"\"\n",
    "        features = self.convnet(x_surname).squeeze(dim=2)\n",
    "        prediction_vector = self.fc(features)\n",
    "\n",
    "        if apply_softmax:\n",
    "            prediction_vector = F.softmax(prediction_vector, dim=1)\n",
    "\n",
    "        return prediction_vector\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_train_state(args):\n",
    "    return {'stop_early': False,\n",
    "            'early_stopping_step': 0,\n",
    "            'early_stopping_best_val': 1e8,\n",
    "            'learning_rate': args.learning_rate,\n",
    "            'epoch_index': 0,\n",
    "            'train_loss': [],\n",
    "            'train_acc': [],\n",
    "            'val_loss': [],\n",
    "            'val_acc': [],\n",
    "            'test_loss': -1,\n",
    "            'test_acc': -1,\n",
    "            'model_filename': args.model_state_file}\n",
    "\n",
    "def update_train_state(args, model, train_state):\n",
    "    \"\"\"Handle the training state updates.\n",
    "\n",
    "    Components:\n",
    "     - Early Stopping: Prevent overfitting.\n",
    "     - Model Checkpoint: Model is saved if the model is better\n",
    "\n",
    "    :param args: main arguments\n",
    "    :param model: model to train\n",
    "    :param train_state: a dictionary representing the training state values\n",
    "    :returns:\n",
    "        a new train_state\n",
    "    \"\"\"\n",
    "\n",
    "    # Save one model at least\n",
    "    if train_state['epoch_index'] == 0:\n",
    "        torch.save(model.state_dict(), train_state['model_filename'])\n",
    "        train_state['stop_early'] = False\n",
    "\n",
    "    # Save model if performance improved\n",
    "    elif train_state['epoch_index'] >= 1:\n",
    "        loss_tm1, loss_t = train_state['val_loss'][-2:]\n",
    "\n",
    "        # If loss worsened\n",
    "        if loss_t >= train_state['early_stopping_best_val']:\n",
    "            # Update step\n",
    "            train_state['early_stopping_step'] += 1\n",
    "        # Loss decreased\n",
    "        else:\n",
    "            # Save the best model\n",
    "            if loss_t < train_state['early_stopping_best_val']:\n",
    "                torch.save(model.state_dict(), train_state['model_filename'])\n",
    "\n",
    "            # Reset early stopping step\n",
    "            train_state['early_stopping_step'] = 0\n",
    "\n",
    "        # Stop early ?\n",
    "        train_state['stop_early'] = \\\n",
    "            train_state['early_stopping_step'] >= args.early_stopping_criteria\n",
    "\n",
    "    return train_state\n",
    "\n",
    "def compute_accuracy(y_pred, y_target):\n",
    "    _, y_pred_indices = y_pred.max(dim=1)\n",
    "    n_correct = torch.eq(y_pred_indices, y_target).sum().item()\n",
    "    return n_correct/len(y_pred_indices) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SurnameClassifier(\n",
      "  (convnet): Sequential(\n",
      "    (0): Conv1d(84, 256, kernel_size=(3,), stride=(1,))\n",
      "    (1): ELU(alpha=1.0)\n",
      "    (2): Conv1d(256, 256, kernel_size=(3,), stride=(2,))\n",
      "    (3): ELU(alpha=1.0)\n",
      "    (4): Conv1d(256, 256, kernel_size=(3,), stride=(2,))\n",
      "    (5): ELU(alpha=1.0)\n",
      "    (6): Conv1d(256, 256, kernel_size=(3,), stride=(1,))\n",
      "    (7): ELU(alpha=1.0)\n",
      "  )\n",
      "  (fc): Linear(in_features=256, out_features=18, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "classifier = SurnameClassifier(initial_num_channels=oh_vectorizer.num_characters, \n",
    "                               num_classes=len(NATIONALITY.vocab.itos),\n",
    "                               num_channels=args.num_channels)\n",
    "print(classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(data_loaders, classifier, \n",
    "          optimizer, oh_vectorizer, loss_func, train_bar):\n",
    "    \"\"\"Train method\n",
    "    \"\"\"\n",
    "    running_loss = 0.0\n",
    "    running_acc = 0.0\n",
    "    classifier.train()\n",
    "    \n",
    "    for batch_index, batch_dict in enumerate(data_loaders[\"train\"]):\n",
    "        # the training routine is these 5 steps:\n",
    "\n",
    "        # --------------------------------------\n",
    "        # step 1. zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # step 2. compute the output\n",
    "\n",
    "        x_in = oh_vectorizer.vectorize(batch_dict.surname)\n",
    "        y_pred = classifier(x_surname=x_in.float())\n",
    "        \n",
    "        # step 3. compute the loss\n",
    "        loss = loss_func(y_pred, batch_dict.nationality.long())\n",
    "        loss_t = loss.item()\n",
    "        running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "\n",
    "        # step 4. use loss to produce gradients\n",
    "        loss.backward()\n",
    "\n",
    "        # step 5. use optimizer to take gradient step\n",
    "        optimizer.step()\n",
    "        # -----------------------------------------\n",
    "        # compute the accuracy\n",
    "        acc_t = compute_accuracy(y_pred, batch_dict.nationality)\n",
    "        running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "\n",
    "        # update bar\n",
    "        train_bar.set_postfix(loss=running_loss, \n",
    "                              acc=running_acc, \n",
    "                              epoch=epoch_index)\n",
    "        train_bar.update()\n",
    "        \n",
    "    return running_loss, running_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val(data_loaders, classifier, oh_vectorizer, loss_func, val_bar):\n",
    "    \"\"\"Validation Method\n",
    "    \"\"\"\n",
    "    running_loss = 0.\n",
    "    running_acc = 0.\n",
    "    classifier.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_index, batch_dict in enumerate(data_loaders[\"val\"]):\n",
    "\n",
    "            # compute the output\n",
    "            x_in = oh_vectorizer.vectorize(batch_dict.surname)\n",
    "            y_pred = classifier(x_in.float())\n",
    "\n",
    "            # step 3. compute the loss\n",
    "            loss = loss_func(y_pred, batch_dict.nationality.long())\n",
    "            loss_t = loss.item()\n",
    "            running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "\n",
    "            # compute the accuracy\n",
    "            acc_t = compute_accuracy(y_pred, batch_dict.nationality)\n",
    "            running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "\n",
    "            val_bar.set_postfix(loss=running_loss, \n",
    "                                acc=running_acc, \n",
    "                                epoch=epoch_index)\n",
    "            val_bar.update()\n",
    "        \n",
    "    return running_loss, running_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "114d82f6d9ae4064bd8f0815d5edd283",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='training routine', style=ProgressStyle(description_width=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "678b11725aa744ae989fe8d160bc6176",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='split=train', max=61.0, style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9f5d72cde4a418cadcf150ff7fe9103",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='split=val', max=13.0, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_loaders = {\"train\": train_iterator, \"val\": val_iterator, \"test\": test_iterator}\n",
    "datasets = {\"train\": train_data, \"val\": val_data, \"test\": test_data}\n",
    "\n",
    "classifier = classifier.to(args.device)\n",
    "weights = 1/torch.tensor([NATIONALITY.vocab.freqs[c] for c in NATIONALITY.vocab.itos], \n",
    "                         dtype=torch.float)\n",
    "loss_func = nn.CrossEntropyLoss(weight=weights)\n",
    "optimizer = optim.Adam(classifier.parameters(), lr=args.learning_rate)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer,\n",
    "                                                 mode='min', factor=0.5,\n",
    "                                                 patience=1)\n",
    "\n",
    "train_state = make_train_state(args)\n",
    "\n",
    "epoch_bar = tqdm_notebook(desc='training routine', \n",
    "                          total=args.num_epochs,\n",
    "                          position=0)\n",
    "\n",
    "train_bar = tqdm_notebook(desc='split=train',\n",
    "                          total=len(data_loaders[\"train\"]), \n",
    "                          position=1, \n",
    "                          leave=True)\n",
    "\n",
    "val_bar = tqdm_notebook(desc='split=val',\n",
    "                        total=len(data_loaders[\"val\"]), \n",
    "                        position=1, \n",
    "                        leave=True)\n",
    "\n",
    "try:\n",
    "    for epoch_index in range(args.num_epochs):\n",
    "        train_state['epoch_index'] = epoch_index\n",
    "\n",
    "        # Iterate over training dataset        \n",
    "        running_loss, running_acc = train(data_loaders, classifier, \n",
    "                                          optimizer, oh_vectorizer, \n",
    "                                          loss_func, train_bar)\n",
    "        \n",
    "        train_state['train_loss'].append(running_loss)\n",
    "        train_state['train_acc'].append(running_acc)\n",
    "\n",
    "        # Iterate over val dataset\n",
    "        running_loss, running_acc = val(data_loaders, classifier, \n",
    "                                        oh_vectorizer, loss_func, \n",
    "                                        val_bar)\n",
    "\n",
    "        train_state['val_loss'].append(running_loss)\n",
    "        train_state['val_acc'].append(running_acc)\n",
    "\n",
    "        train_state = update_train_state(args=args, model=classifier,\n",
    "                                         train_state=train_state)\n",
    "\n",
    "        scheduler.step(train_state['val_loss'][-1])\n",
    "\n",
    "        train_bar.n = 0\n",
    "        val_bar.n = 0\n",
    "        epoch_bar.update()\n",
    "\n",
    "        if train_state['stop_early']:\n",
    "            break\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Exiting loop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c9c237745c14540b1717d567417c8b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=824.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# compute the loss & accuracy on the test set using the best available model\n",
    "# classifier.load_state_dict(torch.load(train_state['model_filename']))\n",
    "classifier = classifier.to(args.device)\n",
    "\n",
    "\n",
    "running_loss = 0.\n",
    "running_acc = 0.\n",
    "classifier.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_index, batch_dict in tqdm_notebook(enumerate(data_loaders[\"test\"]), \n",
    "                                                total = len(data_loaders[\"test\"])):\n",
    "        # compute the output\n",
    "        x_in = oh_vectorizer.vectorize(batch_dict.surname)\n",
    "        y_pred = classifier(x_in.float())\n",
    "\n",
    "        # compute the loss\n",
    "        loss = loss_func(y_pred, batch_dict.nationality.long())\n",
    "        loss_t = loss.item()\n",
    "        running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "\n",
    "        # compute the accuracy\n",
    "        acc_t = compute_accuracy(y_pred, batch_dict.nationality)\n",
    "        running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "\n",
    "train_state['test_loss'] = running_loss\n",
    "train_state['test_acc'] = running_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 1.408\n",
      "Test Accuracy: 58.31\n"
     ]
    }
   ],
   "source": [
    "print(\"Test loss: {:.3f}\".format(train_state['test_loss']))\n",
    "print(\"Test Accuracy: {:.2f}\".format(train_state['test_acc']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_nationality(surname, classifier, surname_field, nationality_field, oh_vectorizer):\n",
    "    \"\"\"Predict the nationality from a new surname\n",
    "    \n",
    "    Args:\n",
    "        surname (str): the surname to classifier\n",
    "        classifier (SurnameClassifer): an instance of the classifier\n",
    "        vectorizer (SurnameVectorizer): the corresponding vectorizer\n",
    "    Returns:\n",
    "        a dictionary with the most likely nationality and its probability\n",
    "    \"\"\"\n",
    "    surname = [char.lower() for char in surname]\n",
    "    surname = surname_field.numericalize([surname])\n",
    "    surname = torch.cat([surname, torch.ones((10, surname.shape[1]), dtype=torch.long)], dim=0)\n",
    "    vectorized_surname = oh_vectorizer.vectorize(surname)\n",
    "    result = classifier(vectorized_surname.float(), apply_softmax=True)\n",
    "\n",
    "    probability_values, indices = result.max(dim=1)\n",
    "    index = indices.item()\n",
    "    \n",
    "\n",
    "    predicted_nationality = nationality_field.vocab.itos[index]\n",
    "    probability_value = probability_values.item()\n",
    "\n",
    "    return {'nationality': predicted_nationality, 'probability': probability_value}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "McMahan -> irish (p=0.95)\n"
     ]
    }
   ],
   "source": [
    "new_surname = \"McMahan\"\n",
    "classifier = classifier.to(\"cpu\")\n",
    "prediction = predict_nationality(new_surname, classifier, SURNAME, NATIONALITY, oh_vectorizer)\n",
    "print(\"{} -> {} (p={:0.2f})\".format(new_surname,\n",
    "                                    prediction['nationality'],\n",
    "                                    prediction['probability']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TopK Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_topk_nationalities(surname, classifier, surname_field, nationality_field, oh_vectorizer, k):\n",
    "    \"\"\"Predict the nationality from a new surname\n",
    "    \n",
    "    Args:\n",
    "        surname (str): the surname to classifier\n",
    "        classifier (SurnameClassifer): an instance of the classifier\n",
    "        vectorizer (SurnameVectorizer): the corresponding vectorizer\n",
    "    Returns:\n",
    "        a dictionary with the most likely nationality and its probability\n",
    "    \"\"\"\n",
    "    surname = [char.lower() for char in surname]\n",
    "    surname = surname_field.numericalize([surname])\n",
    "    surname = torch.cat([surname, torch.ones((10, surname.shape[1]), dtype=torch.long)], dim=0)\n",
    "    \n",
    "    vectorized_surname = oh_vectorizer.vectorize(surname)\n",
    "    result = classifier(vectorized_surname.float(), apply_softmax=True)\n",
    "    probability_values, indices = result.topk(dim=1, k=k)\n",
    "    probability_values = probability_values.view(-1)\n",
    "    index = indices.view(-1)\n",
    "    \n",
    "    results = []\n",
    "    for idx_, idx in enumerate(index):\n",
    "        predicted_nationality = nationality_field.vocab.itos[idx.item()]\n",
    "        probability_value = probability_values[idx_].item()\n",
    "        results.append({'nationality': predicted_nationality, 'probability': probability_value})\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 predictions:\n",
      "===================\n",
      "McMahan -> irish (p=0.95)\n",
      "McMahan -> russian (p=0.02)\n",
      "McMahan -> english (p=0.01)\n",
      "McMahan -> scottish (p=0.01)\n",
      "McMahan -> czech (p=0.01)\n"
     ]
    }
   ],
   "source": [
    "new_surname = \"McMahan\"\n",
    "k = 5\n",
    "classifier = classifier.to(\"cpu\")\n",
    "predictions = predict_topk_nationalities(new_surname, classifier, SURNAME, NATIONALITY, oh_vectorizer, k)\n",
    "\n",
    "print(\"Top {} predictions:\".format(k))\n",
    "print(\"===================\")\n",
    "for prediction in predictions:\n",
    "    print(\"{} -> {} (p={:0.2f})\".format(new_surname,\n",
    "                                        prediction['nationality'],\n",
    "                                        prediction['probability']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
