{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchtext\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchtext.data import (Dataset, Example, Field, \n",
    "                            LabelField, BucketIterator, Iterator)\n",
    "from argparse import Namespace\n",
    "from tqdm.notebook import tqdm as tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    }
   ],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Namespace(\n",
    "    review_csv = \"data/yelp/reviews_with_splits_lite.csv\",\n",
    "    model_state_file='model.pth',\n",
    "    save_dir='model_storage/',\n",
    "    label_field_name = \"rating\",\n",
    "    data_field_name = \"review\",\n",
    "    frequency_cutoff=25,\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu'),\n",
    "    batch_size=128,\n",
    "    early_stopping_criteria=5,\n",
    "    learning_rate=0.001,\n",
    "    num_epochs=100,\n",
    "    seed=1337,\n",
    "    # Runtime options\n",
    "    catch_keyboard_interrupt=True,\n",
    "    expand_filepaths_to_save_dir=True,\n",
    "    reload_from_files=False,\n",
    "    cuda=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Global Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed_everywhere(seed, cuda):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if cuda:\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def handle_dirs(dirpath):\n",
    "    if not os.path.exists(dirpath):\n",
    "        os.makedirs(dirpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expanded filepaths: \n",
      "\tmodel_storage/model.pth\n",
      "Using CUDA: False\n"
     ]
    }
   ],
   "source": [
    "if args.expand_filepaths_to_save_dir:\n",
    "    args.model_state_file = os.path.join(args.save_dir,\n",
    "                                         args.model_state_file)\n",
    "    \n",
    "    print(\"Expanded filepaths: \")\n",
    "    print(\"\\t{}\".format(args.model_state_file))\n",
    "    \n",
    "# Check CUDA\n",
    "if not torch.cuda.is_available():\n",
    "    args.cuda = False\n",
    "\n",
    "print(\"Using CUDA: {}\".format(args.cuda))\n",
    "\n",
    "args.device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
    "\n",
    "# Set seed for reproducibility\n",
    "set_seed_everywhere(args.seed, args.cuda)\n",
    "\n",
    "# handle dirs\n",
    "handle_dirs(args.save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(args.review_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "      <th>review</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>negative</td>\n",
       "      <td>on a recent visit to las vegas , my friends an...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>positive</td>\n",
       "      <td>excellent food ! we had the pompedoro , chicke...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>positive</td>\n",
       "      <td>a great little glimpse back into old vegas . t...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>positive</td>\n",
       "      <td>i was in phoenix for a couple of days for a co...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>positive</td>\n",
       "      <td>what a treasure ! i have been doing yoga for y...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     rating                                             review  split\n",
       "0  negative  on a recent visit to las vegas , my friends an...  train\n",
       "1  positive  excellent food ! we had the pompedoro , chicke...  train\n",
       "2  positive  a great little glimpse back into old vegas . t...  train\n",
       "3  positive  i was in phoenix for a couple of days for a co...  train\n",
       "4  positive  what a treasure ! i have been doing yoga for y...  train"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "def tokenizer(review):\n",
    "    \"\"\"Simple tokenizer\"\"\"\n",
    "    return [word for word in review.split(\" \") \n",
    "            if word not in string.punctuation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "REVIEW = Field(tokenize=tokenizer, sequential=True, lower=True)\n",
    "RATING = LabelField(dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Datasets TorchText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReviewDataset(Dataset):\n",
    "    def __init__(self, review_df, fields):\n",
    "        examples = []\n",
    "        for i, row in tqdm_notebook(review_df.iterrows(), total=review_df.shape[0]):\n",
    "            rating = row.rating\n",
    "            review = row.review\n",
    "            examples.append(Example.fromlist([review, rating], fields))\n",
    "        super().__init__(examples, fields)\n",
    "        \n",
    "    @staticmethod\n",
    "    def sort_key(ex):\n",
    "        return len(ex.review)\n",
    "    \n",
    "    @classmethod\n",
    "    def splits(cls, fields, review_df):\n",
    "        train_data = cls(review_df[review_df.split == \"train\"], fields)\n",
    "        val_data = cls(review_df[review_df.split == \"val\"], fields)\n",
    "        test_data = cls(review_df[review_df.split == \"test\"], fields)\n",
    "        \n",
    "        return train_data, val_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3afac43d01ab475a84fe6f07b721bb00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=39200.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d75022c0a50a4ecb944890d7ff04465e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=8400.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4ffc1eaf42747898f14466940ac412b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=8400.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "fields = [('review', REVIEW), ('rating', RATING)]\n",
    "train_data, val_data, test_data = ReviewDataset.splits(fields, review_df=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REVIEW\n",
      " ['on', 'a', 'recent', 'visit', 'to', 'las', 'vegas', 'my', 'friends', 'and', 'i', 'decided', 'to', 'stay', 'at', 'the', 'monte', 'carlo', 'because', 'it', 'had', 'been', 'recommended', 'to', 'us', 'and', 'we', 'like', 'the', 'location', 'i', 'would', 'say', 'overall', 'that', 'we', 'had', 'a', 'nice', 'vacation', 'but', 'we', 'experienced', 'a', 'problem', 'at', 'the', 'end', 'of', 'our', 'stay', 'n', 'nafter', 'we', 'had', 'packed', 'our', 'bags', 'and', 'just', 'before', 'checking', 'out', 'i', 'called', 'for', 'a', 'bellman', 'to', 'take', 'our', 'bags', 'down', 'for', 'us', 'after', 'checking', 'out', 'and', 'driving', 'home', 'we', 'discovered', 'that', 'a', 'bottle', 'of', 'perfume', 'had', 'been', 'broken', 'in', 'one', 'of', 'our', 'bags', 'as', 'all', 'we', 'did', 'was', 'go', 'straight', 'from', 'the', 'hotel', 'back', 'to', 'our', 'house', 'we', 'concluded', 'the', 'bag', 'had', 'most', 'likely', 'been', 'mishandled', 'by', 'the', 'bellman', 'n', 'nwe', 'immediately', 'called', 'the', 'hotel', 'and', 'a', 'very', 'sympathetic', 'person', 'took', 'our', 'information', 'and', 'told', 'us', 'he', 'would', 'investigate', 'what', 'happened', 'as', 'well', 'as', 'what', 'they', 'could', 'do', 'fo', 'us', 'and', 'get', 'back', 'to', 'us', 'within', 'hours', 'after', 'waiting', 'for', 'days', 'without', 'a', 'response', 'i', 'called', 'back', 'to', 'follow', 'up', 'i', 'was', 'unable', 'to', 'speak', 'to', 'the', 'person', 'who', 'originally', 'helped', 'me', 'and', 'found', 'that', 'the', 'person', 'with', 'whom', 'i', 'was', 'now', 'speaking', 'had', 'a', 'very', 'unpleasant', 'almost', 'accusing', 'tone', 'in', 'her', 'voice', 'n', 'nshe', 'informed', 'me', 'that', 'there', 'was', 'nothing', 'that', 'they', 'would', 'be', 'able', 'to', 'do', 'insisting', 'that', 'the', 'negligence', 'lay', 'with', 'us', 'we', 'were', 'also', 'advised', 'that', 'the', 'first', 'person', 'with', 'whom', 'we', 'spoke', 'should', 'not', 'have', 'committed', 'to', 'returning', 'our', 'call', 'within', 'hours', 'as', 'this', 'was', 'not', 'their', 'policy', 'n', 'noverall', 'i', 'would', 'say', 'that', 'i', 'was', 'disappointed', 'by', 'the', 'fact', 'that', 'they', 'are', 'not', 'willing', 'to', 'do', 'anything', 'about', 'the', 'broken', 'bottle', 'of', 'perfume', 'which', 'spilled', 'all', 'over', 'an', 'expensive', 'piece', 'of', 'luggage', 'but', 'even', 'more', 'unsatisfied', 'by', 'the', 'disingenuous', 'manner', 'in', 'which', 'we', 'were', 'treated', 'it', 'was', 'an', 'unpleasant', 'end', 'to', 'our', 'vacation', 'and', 'i', 'certainly', 'have', 'no', 'plans', 'to', 'revisit', 'the', 'hotel', 'nor', 'will', 'i', 'recommend', 'it', 'to', 'anyone', 'i', 'know']\n",
      "RATING\n",
      " negative\n"
     ]
    }
   ],
   "source": [
    "print(\"REVIEW\\n\",vars(train_data[0])[\"review\"])\n",
    "print(\"RATING\\n\",vars(train_data[0])[\"rating\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "REVIEW.build_vocab(train_data, min_freq=args.frequency_cutoff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "RATING.build_vocab(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'freqs': Counter({'negative': 19600, 'positive': 19600}),\n",
       " 'itos': ['negative', 'positive'],\n",
       " 'unk_index': None,\n",
       " 'stoi': defaultdict(None, {'negative': 0, 'positive': 1}),\n",
       " 'vectors': None}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vars(RATING.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOTAL_WORDS =  7712\n"
     ]
    }
   ],
   "source": [
    "print(\"TOTAL_WORDS = \", len(REVIEW.vocab.itos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<unk>', '<pad>']\n"
     ]
    }
   ],
   "source": [
    "print(REVIEW.vocab.itos[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.total_words = len(REVIEW.vocab.itos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create One-Hot-Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vectorizer(object):\n",
    "    def __init__(self, review_field):\n",
    "        self._review_field = review_field\n",
    "        self._slicing = torch.tensor([i for i in # Exclude <pad> token\n",
    "                                      range(len(self._review_field.vocab.itos)) if i !=1])\n",
    "        self._dimension = len(self._slicing)\n",
    "    \n",
    "    def vectorize(self, batch_matrix):\n",
    "        batch_size = batch_matrix.shape[1]\n",
    "        one_hot = torch.zeros((batch_size, len(self._review_field.vocab.itos)), dtype=torch.long)\n",
    "        indices = batch_matrix.T\n",
    "        source = torch.ones_like(indices)\n",
    "        # source = (indices != 1).long() # Exclude <pad> token\n",
    "        one_hot.scatter_(1, indices, source)\n",
    "        return one_hot[:, self._slicing] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "oh_vectorize = Vectorizer(REVIEW)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Iterators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iterator, val_iterator = BucketIterator.splits(\n",
    "    (train_data, val_data), \n",
    "    batch_size = args.batch_size,\n",
    "    sort_within_batch = True,\n",
    "    device = args.device,\n",
    ")\n",
    "\n",
    "test_iterator = Iterator(test_data, batch_size=args.batch_size,\n",
    "                        device=args.device, sort=False, sort_within_batch=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [0, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])\n",
      "<unk> <unk> <unk> <unk> the the the the the the and and and and i i to to a a a a a a a was was was it it of of for for in in n n is that that this with t have have s had at at at at at were so there out if if get would back their an what only us been because ni ni also well over better best come chicken minutes room think bad table table took wasn long hour why nwe least breakfast waiting waiting waiting tasted served shrimp sat may rather sitting under rest eggs eggs spend simple mediocre hope happened nservice cocktail heat smile nfood shit ham dude nwell golden gate pig explanation lamp blackjack laying\n",
      "\n",
      "\n",
      "<pad> the and i to a was it of for in n is that this with t have s had at were so there out if get would back their an what only us been because ni also well over better best come chicken minutes room think bad table took wasn long hour why nwe least breakfast waiting tasted served shrimp sat may rather sitting under rest eggs spend simple mediocre hope happened nservice cocktail heat smile nfood shit ham dude nwell golden gate pig explanation lamp blackjack laying\n"
     ]
    }
   ],
   "source": [
    "for batch in train_iterator:\n",
    "    v_code = oh_vectorize.vectorize(batch.review)\n",
    "    print(v_code)\n",
    "    first = batch.review[:,0].sort().values\n",
    "    decode = ' '.join([REVIEW.vocab.itos[idx] for idx in first if idx != 1])\n",
    "    print(decode)\n",
    "    first = v_code[0]\n",
    "    decode = ' '.join([REVIEW.vocab.itos[idx] for idx, val in enumerate(first, 1)\n",
    "                      if val!= 0])\n",
    "    print(\"\\n\")\n",
    "    print(decode)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReviewClassifier(nn.Module):\n",
    "    \"\"\" a simple perceptron based classifier \"\"\"\n",
    "    def __init__(self, num_features):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            num_features (int): the size of the input feature vector\n",
    "        \"\"\"\n",
    "        super(ReviewClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(in_features=num_features, \n",
    "                             out_features=1)\n",
    "\n",
    "    def forward(self, x_in, apply_sigmoid=False):\n",
    "        \"\"\"The forward pass of the classifier\n",
    "        \n",
    "        Args:\n",
    "            x_in (torch.Tensor): an input data tensor. \n",
    "                x_in.shape should be (batch, num_features)\n",
    "            apply_sigmoid (bool): a flag for the sigmoid activation\n",
    "                should be false if used with the Cross Entropy losses\n",
    "        Returns:\n",
    "            the resulting tensor. tensor.shape should be (batch,)\n",
    "        \"\"\"\n",
    "        y_out = self.fc1(x_in.squeeze()).squeeze()\n",
    "        if apply_sigmoid:\n",
    "            y_out = torch.sigmoid(y_out)\n",
    "        return y_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_train_state(args):\n",
    "    return {'stop_early': False,\n",
    "            'early_stopping_step': 0,\n",
    "            'early_stopping_best_val': 1e8,\n",
    "            'learning_rate': args.learning_rate,\n",
    "            'epoch_index': 0,\n",
    "            'train_loss': [],\n",
    "            'train_acc': [],\n",
    "            'val_loss': [],\n",
    "            'val_acc': [],\n",
    "            'test_loss': -1,\n",
    "            'test_acc': -1,\n",
    "            'model_filename': args.model_state_file}\n",
    "\n",
    "def update_train_state(args, model, train_state):\n",
    "    \"\"\"Handle the training state updates.\n",
    "\n",
    "    Components:\n",
    "     - Early Stopping: Prevent overfitting.\n",
    "     - Model Checkpoint: Model is saved if the model is better\n",
    "\n",
    "    :param args: main arguments\n",
    "    :param model: model to train\n",
    "    :param train_state: a dictionary representing the training state values\n",
    "    :returns:\n",
    "        a new train_state\n",
    "    \"\"\"\n",
    "\n",
    "    # Save one model at least\n",
    "    if train_state['epoch_index'] == 0:\n",
    "        torch.save(model.state_dict(), train_state['model_filename'].format(train_state['epoch_index']))\n",
    "        train_state['stop_early'] = False\n",
    "\n",
    "    # Save model if performance improved\n",
    "    elif train_state['epoch_index'] >= 1:\n",
    "        loss_tm1, loss_t = train_state['val_loss'][-2:]\n",
    "\n",
    "        # If loss worsened\n",
    "        if loss_t >= train_state['early_stopping_best_val']:\n",
    "            # Update step\n",
    "            train_state['early_stopping_step'] += 1\n",
    "        # Loss decreased\n",
    "        else:\n",
    "            # Save the best model\n",
    "            if loss_t < train_state['early_stopping_best_val']:\n",
    "                torch.save(model.state_dict(), train_state['model_filename'])\n",
    "\n",
    "            # Reset early stopping step\n",
    "            train_state['early_stopping_step'] = 0\n",
    "\n",
    "        # Stop early ?\n",
    "        train_state['stop_early'] = \\\n",
    "            train_state['early_stopping_step'] >= args.early_stopping_criteria\n",
    "\n",
    "    return train_state\n",
    "\n",
    "def compute_accuracy(y_pred, y_target):\n",
    "    y_target = y_target.cpu()\n",
    "    y_pred_indices = (torch.sigmoid(y_pred)>0.5).cpu().long()#.max(dim=1)[1]\n",
    "    n_correct = torch.eq(y_pred_indices, y_target).sum().item()\n",
    "    return n_correct / len(y_pred_indices) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initilization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ReviewClassifier(\n",
       "  (fc1): Linear(in_features=7711, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = ReviewClassifier(num_features=oh_vectorize._dimension)\n",
    "classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(data_loaders, classifier, optimizer, loss_func, train_bar):\n",
    "    running_loss = 0.0\n",
    "    running_acc = 0.0\n",
    "    classifier.train()\n",
    "\n",
    "    for batch_index, batch_dict in enumerate(data_loaders[\"train\"]):\n",
    "        # the training routine is these 5 steps:\n",
    "\n",
    "        # --------------------------------------\n",
    "        # step 1. zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # step 2. compute the output\n",
    "\n",
    "        x_in = oh_vectorize.vectorize(batch_dict.review)\n",
    "        y_pred = classifier(x_in=x_in.float())\n",
    "\n",
    "        # step 3. compute the loss\n",
    "        loss = loss_func(y_pred, batch_dict.rating)\n",
    "        loss_t = loss.item()\n",
    "        running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "\n",
    "        # step 4. use loss to produce gradients\n",
    "        loss.backward()\n",
    "\n",
    "        # step 5. use optimizer to take gradient step\n",
    "        optimizer.step()\n",
    "        # -----------------------------------------\n",
    "        # compute the accuracy\n",
    "        acc_t = compute_accuracy(y_pred, batch_dict.rating)\n",
    "        running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "\n",
    "        # update bar\n",
    "        train_bar.set_postfix(loss=running_loss, \n",
    "                              acc=running_acc, \n",
    "                              epoch=epoch_index)\n",
    "        train_bar.update()\n",
    "\n",
    "    return running_loss, running_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val(data_loaders, classifier, loss_func, val_bar):\n",
    "    running_loss = 0.\n",
    "    running_acc = 0.\n",
    "    classifier.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_index, batch_dict in enumerate(data_loaders[\"val\"]):\n",
    "\n",
    "            # compute the output\n",
    "            x_in = oh_vectorize.vectorize(batch_dict.review)\n",
    "            y_pred = classifier(x_in=x_in.float())\n",
    "\n",
    "            # step 3. compute the loss\n",
    "            loss = loss_func(y_pred, batch_dict.rating)\n",
    "            loss_t = loss.item()\n",
    "            running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "\n",
    "            # compute the accuracy\n",
    "            acc_t = compute_accuracy(y_pred, batch_dict.rating)\n",
    "            running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "\n",
    "            val_bar.set_postfix(loss=running_loss, \n",
    "                                acc=running_acc, \n",
    "                                epoch=epoch_index)\n",
    "            val_bar.update()\n",
    "    return running_loss, running_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loaders = {\"train\": train_iterator, \"val\": val_iterator, \"test\": test_iterator}\n",
    "datasets = {\"train\": train_data, \"val\": val_data, \"test\": test_data}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33d9f6c6d3d0489f998aa2a5c07b46dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='training routine', style=ProgressStyle(description_width=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed08b1fe9d234f63a63904df063138ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='split=train', max=307.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cb93278a2a74c1fadf76f86cdf1ac5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='split=val', max=66.0, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "classifier = classifier.to(args.device)\n",
    "\n",
    "loss_func = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(classifier.parameters(), lr=args.learning_rate)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer,\n",
    "                                                 mode='min', factor=0.5,\n",
    "                                                 patience=1)\n",
    "\n",
    "train_state = make_train_state(args)\n",
    "\n",
    "epoch_bar = tqdm_notebook(desc='training routine', \n",
    "                          total=args.num_epochs,\n",
    "                          position=0)\n",
    "\n",
    "train_bar = tqdm_notebook(desc='split=train',\n",
    "                          total=len(data_loaders[\"train\"]), \n",
    "                          position=1, \n",
    "                          leave=True)\n",
    "\n",
    "val_bar = tqdm_notebook(desc='split=val',\n",
    "                        total=len(data_loaders[\"val\"]), \n",
    "                        position=1, \n",
    "                        leave=True)\n",
    "\n",
    "try:\n",
    "    for epoch_index in range(args.num_epochs):\n",
    "        train_state['epoch_index'] = epoch_index\n",
    "\n",
    "        # Iterate over training dataset\n",
    "        running_loss, running_acc = train(data_loaders, classifier, \n",
    "                                          optimizer, loss_func, train_bar)\n",
    "        \n",
    "        writer.add_scalar('Loss/train', running_loss, epoch_index)\n",
    "        writer.add_scalar('Accuracy/train', running_acc, epoch_index)\n",
    "        \n",
    "        train_state['train_loss'].append(running_loss)\n",
    "        train_state['train_acc'].append(running_acc)\n",
    "\n",
    "        # Iterate over val dataset\n",
    "        running_loss, running_acc = val(data_loaders, classifier,\n",
    "                                       loss_func, val_bar)\n",
    "        \n",
    "        writer.add_scalar('Loss/val', running_loss, epoch_index)\n",
    "        writer.add_scalar('Accuracy/val', running_acc, epoch_index)\n",
    "        \n",
    "        train_state['val_loss'].append(running_loss)\n",
    "        train_state['val_acc'].append(running_acc)\n",
    "\n",
    "        train_state = update_train_state(args=args, model=classifier,\n",
    "                                         train_state=train_state)\n",
    "\n",
    "        scheduler.step(train_state['val_loss'][-1])\n",
    "\n",
    "        train_bar.n = 0\n",
    "        val_bar.n = 0\n",
    "        epoch_bar.update()\n",
    "\n",
    "        if train_state['stop_early']:\n",
    "            break\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Exiting loop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6007 (pid 43615), started 0:00:36 ago. (Use '!kill 43615' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-a22b6d9573d4da7f\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-a22b6d9573d4da7f\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          url.port = 6007;\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ad77a3a091d4964a1a649e41e2ef579",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=66.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# compute the loss & accuracy on the test set using the best available model\n",
    "\n",
    "classifier.load_state_dict(torch.load(train_state['model_filename']))\n",
    "classifier = classifier.to(args.device)\n",
    "\n",
    "\n",
    "running_loss = 0.\n",
    "running_acc = 0.\n",
    "classifier.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_index, batch_dict in tqdm_notebook(enumerate(data_loaders[\"test\"]), \n",
    "                                                total = len(data_loaders[\"test\"])):\n",
    "        # compute the output\n",
    "        x_in = oh_vectorize.vectorize(batch_dict.review)\n",
    "        y_pred = classifier(x_in=x_in.float())\n",
    "\n",
    "        # compute the loss\n",
    "        loss = loss_func(y_pred, batch_dict.rating.float())\n",
    "        loss_t = loss.item()\n",
    "        running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "\n",
    "        # compute the accuracy\n",
    "        acc_t = compute_accuracy(y_pred, batch_dict.rating)\n",
    "        running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "\n",
    "train_state['test_loss'] = running_loss\n",
    "train_state['test_acc'] = running_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.207\n",
      "Test Accuracy: 92.03\n"
     ]
    }
   ],
   "source": [
    "print(\"Test loss: {:.3f}\".format(train_state['test_loss']))\n",
    "print(\"Test Accuracy: {:.2f}\".format(train_state['test_acc']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"([.,!?])\", r\" \\1 \", text)\n",
    "    text = re.sub(r\"[^a-zA-Z.,!?]+\", r\" \", text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_rating(review, classifier, review_field, rating_field,\n",
    "                   oh_vectorizer, decision_threshold=0.5):\n",
    "    \"\"\"Predict the rating of a review\n",
    "    \n",
    "    Args:\n",
    "        review (str): the text of the review\n",
    "        classifier (ReviewClassifier): the trained model\n",
    "        vectorizer (ReviewVectorizer): the corresponding vectorizer\n",
    "        decision_threshold (float): The numerical boundary which separates the rating classes\n",
    "    \"\"\"\n",
    "    review = preprocess_text(review)\n",
    "    review = review_field.tokenize(review)\n",
    "    review = review_field.numericalize([review])\n",
    "    vectorized_review = oh_vectorizer.vectorize(review)\n",
    "    \n",
    "    result = classifier(vectorized_review.float())\n",
    "    \n",
    "    probability_value = torch.sigmoid(result).item()\n",
    "    index = 1\n",
    "    if probability_value < decision_threshold:\n",
    "        index = 0\n",
    "        \n",
    "    return rating_field.itos[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This book is meh -> negative\n"
     ]
    }
   ],
   "source": [
    "test_review = \"This book is meh\"\n",
    "\n",
    "classifier = classifier.cpu()\n",
    "prediction = predict_rating(test_review, classifier, REVIEW, RATING.vocab, oh_vectorize, decision_threshold=0.5)\n",
    "print(\"{} -> {}\".format(test_review, prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 7711])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.fc1.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookup_index = ['<unk>'] + REVIEW.vocab.itos[2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Influential words in Positive Reviews:\n",
      "--------------------------------------\n",
      "excellent\n",
      "delicious\n",
      "amazing\n",
      "disappoint\n",
      "outstanding\n",
      "perfect\n",
      "awesome\n",
      "perfection\n",
      "yum\n",
      "incredible\n",
      "fantastic\n",
      "ngreat\n",
      "great\n",
      "downside\n",
      "superb\n",
      "heaven\n",
      "love\n",
      "hooked\n",
      "wonderful\n",
      "perfectly\n",
      "====\n",
      "\n",
      "\n",
      "\n",
      "Influential words in Negative Reviews:\n",
      "--------------------------------------\n",
      "worst\n",
      "horrible\n",
      "terrible\n",
      "bland\n",
      "awful\n",
      "meh\n",
      "mediocre\n",
      "tasteless\n",
      "poisoning\n",
      "disgusting\n",
      "poor\n",
      "eh\n",
      "overpriced\n",
      "disappointment\n",
      "inedible\n",
      "rude\n",
      "disappointing\n",
      "lacked\n",
      "sucks\n",
      "overrated\n"
     ]
    }
   ],
   "source": [
    "# Sort weights\n",
    "fc1_weights = classifier.fc1.weight.detach()[0]\n",
    "_, indices = torch.sort(fc1_weights, dim=0, descending=True)\n",
    "indices = indices.numpy().tolist()\n",
    "\n",
    "# Top 20 words\n",
    "print(\"Influential words in Positive Reviews:\")\n",
    "print(\"--------------------------------------\")\n",
    "for i in range(20):\n",
    "    print(lookup_index[indices[i]])\n",
    "    \n",
    "print(\"====\\n\\n\\n\")\n",
    "\n",
    "# Top 20 negative words\n",
    "print(\"Influential words in Negative Reviews:\")\n",
    "print(\"--------------------------------------\")\n",
    "indices.reverse()\n",
    "for i in range(20):\n",
    "    print(lookup_index[indices[i]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
